{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math, time, json, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/path/to/noisy_dataset.npy\"\n",
    "arr = np.load(path)\n",
    "assert arr.ndim == 3 and arr.shape == (4, 300, 4), f\"Expected (4,300,4), got {arr.shape}\"\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "for i in range(arr.shape[0]):   \n",
    "    traj = arr[i]  \n",
    "    \n",
    "    state1 = traj[:, 0]  \n",
    "    state2 = traj[:, 1]  \n",
    "    \n",
    "    plt.plot(\n",
    "        state1, state2,\n",
    "        linewidth=2.5,   \n",
    "        label=f\"Trajectory {i+1}\"\n",
    "    )\n",
    "\n",
    "# Bold title\n",
    "plt.title(\"Clean Trajectories\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Bold axis labels\n",
    "plt.xlabel(\"X axis\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"Y axis\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "# Bold tick labels\n",
    "plt.xticks(fontsize=10, fontweight=\"bold\")\n",
    "plt.yticks(fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# Fix axis range to [-2, 2]\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "\n",
    "# Grid and aspect ratio\n",
    "plt.grid(True, linewidth=1.2)\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "# plt.legend(fontsize=10, frameon=True)\n",
    "\n",
    "# Save high-resolution image\n",
    "plt.savefig(\"noisy_trajectories.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demos = []\n",
    "for i in range(arr.shape[0]):\n",
    "    S = arr[i,:, :2].astype(np.float32)\n",
    "    A = arr[i,:, 2:4].astype(np.float32)\n",
    "    demos.append({\"states\": S, \"actions_exec\": A})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "Xs, Ys, Ids = [], [], []\n",
    "off = 0\n",
    "for i, d in enumerate(demos):\n",
    "    S = d[\"states\"]; A = d[\"actions_exec\"]\n",
    "    Xs.append(S); Ys.append(A)\n",
    "    Ids.append(np.full((S.shape[0],), i, np.int64))\n",
    "X = np.concatenate(Xs, axis=0)\n",
    "Y = np.concatenate(Ys, axis=0)\n",
    "demo_ids = np.concatenate(Ids, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize\n",
    "def normalize_xy(X, Y):\n",
    "    x_mean, x_std = X.mean(0), X.std(0) + 1e-6\n",
    "    y_mean, y_std = Y.mean(0), Y.std(0) + 1e-6\n",
    "    Xn = (X - x_mean) / x_std\n",
    "    Yn = (Y - y_mean) / y_std\n",
    "    stats = dict(x_mean=x_mean, x_std=x_std, y_mean=y_mean, y_std=y_std)\n",
    "    return Xn, Yn, stats\n",
    "\n",
    "Xn, Yn, stats = normalize_xy(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(*arrs, device=\"cpu\"):\n",
    "    out=[]\n",
    "    for a in arrs:\n",
    "        if isinstance(a,np.ndarray) and a.dtype==np.int64:\n",
    "            out.append(torch.tensor(a,dtype=torch.long,device=device))\n",
    "        else:\n",
    "            out.append(torch.tensor(a,dtype=torch.float32,device=device))\n",
    "    return out\n",
    "\n",
    "device = \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, p, d, width=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(p, width), nn.ReLU(),\n",
    "            nn.Linear(width, width), nn.ReLU(),\n",
    "            nn.Linear(width, d),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class GaussianMLP(nn.Module):\n",
    "    def __init__(self, p, d, width=128, min_std=1e-3):\n",
    "        super().__init__()\n",
    "        self.min_std = min_std\n",
    "        self.mu_head = nn.Sequential(\n",
    "            nn.Linear(p, width), nn.ReLU(),\n",
    "            nn.Linear(width, width), nn.ReLU(),\n",
    "            nn.Linear(width, d)\n",
    "        )\n",
    "        self.log_std_head = nn.Sequential(\n",
    "            nn.Linear(p, width), nn.ReLU(),\n",
    "            nn.Linear(width, width), nn.ReLU(),\n",
    "            nn.Linear(width, d)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        mu = self.mu_head(x)\n",
    "        log_std = self.log_std_head(x)\n",
    "        std = F.softplus(log_std) + self.min_std\n",
    "        return D.Independent(D.Normal(loc=mu, scale=std), 1)\n",
    "\n",
    "class GMMMLP(nn.Module):\n",
    "    def __init__(self, p, d, M=5, width=128, min_std=1e-3):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.d = d\n",
    "        self.min_std = min_std\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(p, width), nn.ReLU(),\n",
    "            nn.Linear(width, width), nn.ReLU()\n",
    "        )\n",
    "        self.pi_head = nn.Linear(width, M)\n",
    "        self.mu_head = nn.Linear(width, M * d)\n",
    "        self.log_std_head = nn.Linear(width, M * d)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        B = x.shape[0]\n",
    "        pi_logits = self.pi_head(h)\n",
    "        mu = self.mu_head(h).view(B, self.M, self.d)\n",
    "        log_std = self.log_std_head(h).view(B, self.M, self.d)\n",
    "        std = F.softplus(log_std) + self.min_std\n",
    "        \n",
    "        comp_dist = D.Independent(D.Normal(loc=mu, scale=std), 1)\n",
    "        mix_dist = D.Categorical(logits=pi_logits)\n",
    "        return D.MixtureSameFamily(mix_dist, comp_dist)\n",
    "\n",
    "class GRUPolicy(nn.Module):\n",
    "    def __init__(self, p, d, h=128):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(p, h, batch_first=True)\n",
    "        self.head = nn.Linear(h, d)\n",
    "    def forward(self, x, h0=None):\n",
    "        o, h = self.gru(x, h0); y = self.head(o); return y, h\n",
    "\n",
    "class TransformerPolicy(nn.Module):\n",
    "    def __init__(self, p, d, nhead=2, nlayers=2, dim_feedforward=256, dropout=0.1, context_length=32):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.d = d\n",
    "        self.context_length = context_length\n",
    "        self.input_proj = nn.Linear(p, dim_feedforward)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_feedforward, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "        self.output_head = nn.Linear(dim_feedforward, d)\n",
    "    def forward(self, x):\n",
    "        h = self.input_proj(x)\n",
    "        h = self.transformer_encoder(h)\n",
    "        return self.output_head(h)\n",
    "\n",
    "class ILEEDGMM(nn.Module):\n",
    "    def __init__(self, p, d, n_demos, M=5, k=16, width=128, rho_min=0.05,\n",
    "                 l2_omega=1e-3, l2_embed=1e-4, aux_lambda=1e-2):\n",
    "        super().__init__()\n",
    "        self.p, self.d, self.M, self.k = p, d, M, k\n",
    "        self.width = width\n",
    "        self.rho_min = rho_min\n",
    "        self.l2_omega = l2_omega\n",
    "        self.l2_embed = l2_embed\n",
    "        self.aux_lambda = aux_lambda\n",
    "        hid = width\n",
    "        self.feat = nn.Sequential(nn.Linear(p,hid), nn.ReLU(),\n",
    "                                  nn.Linear(hid,hid), nn.ReLU())\n",
    "        self.pi_head   = nn.Linear(hid, M)\n",
    "        self.mu_head   = nn.Linear(hid, M*d)\n",
    "        self.logsig_head = nn.Linear(hid, M*d)\n",
    "        self.embed = nn.Sequential(nn.Linear(p,width), nn.ReLU(), nn.Linear(width,k))\n",
    "        self.omega = nn.Parameter(torch.zeros(n_demos,k))\n",
    "        nn.init.normal_(self.omega, std=0.1)\n",
    "        self.trans = nn.Sequential(\n",
    "            nn.Linear(k + d, width), nn.ReLU(),\n",
    "            nn.Linear(width, width), nn.ReLU(),\n",
    "            nn.Linear(width, k),\n",
    "        )\n",
    "        self.min_sigma = 1e-3\n",
    "    def gmm_params(self, s):\n",
    "        h = self.feat(s)\n",
    "        B = s.shape[0]\n",
    "        pi_logits = self.pi_head(h)\n",
    "        mu = self.mu_head(h).view(B, self.M, self.d)\n",
    "        log_sigma = self.logsig_head(h).view(B, self.M, self.d)\n",
    "        sigma = torch.nn.functional.softplus(log_sigma) + self.min_sigma\n",
    "        return pi_logits, mu, sigma\n",
    "    def rho_from_ids(self, s_for_embed, demo_ids):\n",
    "        z = self.embed(s_for_embed)\n",
    "        w = self.omega[demo_ids]\n",
    "        rho = torch.sigmoid((z*w).sum(-1, keepdim=True))\n",
    "        rho = torch.clamp(rho, min=self.rho_min, max=1.0)\n",
    "        return rho, z\n",
    "    def logprob(self, a, pi_logits, mu, sigma, rho):\n",
    "        B,M,d = a.shape[0], self.M, self.d\n",
    "        a_exp = a[:,None,:].expand(B,M,d)\n",
    "        quad_base = ((a_exp - mu) / (sigma+1e-8))**2\n",
    "        quad_base = quad_base.sum(-1)\n",
    "        s2log = (2.0*torch.log(sigma+1e-8)).sum(-1)\n",
    "        logdet_eff = s2log - d * torch.log(rho+1e-8)\n",
    "        const = d * math.log(2*math.pi)\n",
    "        logN = -0.5*(const + logdet_eff + rho*quad_base)\n",
    "        logpi = torch.log_softmax(pi_logits, dim=-1)\n",
    "        logmix = logpi + logN\n",
    "        return torch.logsumexp(logmix, dim=-1)\n",
    "    def nll_with_aux(self, s_policy, s_embed, a, demo_ids, s_next, aux_lambda=None):\n",
    "        pi_logits, mu, sigma = self.gmm_params(s_policy)\n",
    "        rho, z = self.rho_from_ids(s_embed, demo_ids)\n",
    "        logp = self.logprob(a, pi_logits, mu, sigma, rho)\n",
    "        nll = -logp.mean()\n",
    "        if aux_lambda is None:\n",
    "            aux_lambda = self.aux_lambda\n",
    "        with torch.no_grad():\n",
    "            z_next_target = self.embed(s_next)\n",
    "        z_pred = self.trans(torch.cat([z, a], dim=-1))\n",
    "        aux = ((z_pred - z_next_target)**2).mean()\n",
    "        reg = self.l2_omega*(self.omega**2).mean() + self.l2_embed*(z**2).mean()\n",
    "        return nll + aux_lambda*aux + reg, dict(nll=nll.detach(), aux=aux.detach(), reg=reg.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq_dataset(demos, Xn, Yn, seq_len=32, seq_stride=16):\n",
    "    seqs=[]; off=0\n",
    "    for d in demos:\n",
    "        T=d[\"states\"].shape[0]\n",
    "        xs=Xn[off:off+T]; ys=Yn[off:off+T]; off+=T\n",
    "        for s in range(0, max(1,T-seq_len+1), seq_stride):\n",
    "            e=min(T,s+seq_len); seqs.append((xs[s:e], ys[s:e]))\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transition_dataset(demos, Xn, Yn):\n",
    "    S=[]; A=[]; S2=[]; ids=[]\n",
    "    off=0\n",
    "    for i,d in enumerate(demos):\n",
    "        T=d[\"states\"].shape[0]\n",
    "        if T<2: off+=T; continue\n",
    "        S.append(Xn[off:off+T-1])\n",
    "        A.append(Yn[off:off+T-1])\n",
    "        S2.append(Xn[off+1:off+T])\n",
    "        ids.append(np.full((T-1,), i, np.int64))\n",
    "        off+=T\n",
    "    if len(S)==0:\n",
    "        return np.zeros((0,Xn.shape[1]),np.float32), np.zeros((0,Yn.shape[1]),np.float32), np.zeros((0,Xn.shape[1]),np.float32), np.zeros((0,),np.int64)\n",
    "    return np.concatenate(S,0), np.concatenate(A,0), np.concatenate(S2,0), np.concatenate(ids,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bc(Xn, Yn, width=128, W=None, lr=1e-3, epochs=120, batch=8192, device=\"cpu\", clip=1.0):\n",
    "    p,d = Xn.shape[1], Yn.shape[1]\n",
    "    net = MLP(p,d,width=width).to(device)\n",
    "    opt = optim.Adam(net.parameters(), lr=lr)\n",
    "    Xt,Yt = to_torch(Xn,Yn,device=device)\n",
    "    Wt = torch.ones(Xt.shape[0],device=device) if W is None else torch.tensor(W,dtype=torch.float32,device=device)\n",
    "    N=Xt.shape[0]; B=min(batch,N)\n",
    "    for _ in range(epochs):\n",
    "        idx=torch.randperm(N,device=device)\n",
    "        for k in range(0,N,B):\n",
    "            sel=idx[k:k+B]; x=Xt[sel]; y=Yt[sel]; w=Wt[sel]\n",
    "            pred=net(x); mse=((pred-y)**2).sum(-1); loss=(w*mse).mean()\n",
    "            opt.zero_grad(); loss.backward()\n",
    "            if clip: nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "    # final training loss (same objective)\n",
    "    with torch.no_grad():\n",
    "        pred = net(Xt); mse = ((pred - Yt)**2).sum(-1)\n",
    "        loss = (Wt * mse).mean().item()\n",
    "    return net, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_traj_bc(demos, Xn, Yn, width=128, lr=1e-3, epochs=120, batch=8192, device=\"cpu\", clip=1.0):\n",
    "    # per-trajectory weights (same heuristic)\n",
    "    demo_w=[]\n",
    "    for d in demos:\n",
    "        slip_frac=float(d.get(\"slip_width\", 0.25))\n",
    "        clip_ratio=float(d.get(\"clip_ratio\", 0.05))\n",
    "        dist=float(d.get(\"distance\", 8.0))\n",
    "        w=(1.0 - 0.5*slip_frac) * (1.0 - 0.3*clip_ratio) * (1.0 + 0.02*dist)\n",
    "        demo_w.append(w)\n",
    "    demo_w=np.array(demo_w,np.float32); demo_w/=demo_w.mean()\n",
    "    W=np.concatenate([np.full(d[\"states\"].shape[0], w, np.float32) for w,d in zip(demo_w, demos)],0)\n",
    "    net, loss = train_bc(Xn, Yn, width=width, W=W, lr=lr, epochs=epochs, batch=batch, device=device, clip=clip)\n",
    "    return net, loss, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ileed_paperfaithful(demos, Xn, Yn, demo_ids, width=128, k=16, M=5, rho_min=0.05,\n",
    "                              l2_omega=1e-3, l2_embed=1e-4, aux_lambda=1e-2,\n",
    "                              epochs_warm=60, epochs_joint=90, lr_warm=1e-3, lr_joint=6e-4,\n",
    "                              batch=256, grad_clip=1.0, device=\"cpu\"):\n",
    "    p, d = Xn.shape[1], Yn.shape[1]; n_d = len(demos)\n",
    "    net = ILEEDGMM(p, d, n_d, M=M, k=k, width=width, rho_min=rho_min,\n",
    "                   l2_omega=l2_omega, l2_embed=l2_embed, aux_lambda=aux_lambda).to(device)\n",
    "    S0, A0, S1, ids_tr = make_transition_dataset(demos, Xn, Yn)\n",
    "    Xt, At, Xnext, Ids_tr = to_torch(S0, A0, S1, ids_tr, device=device)\n",
    "    Ntr = Xt.shape[0]; B = min(batch, Ntr)\n",
    "    # Warm\n",
    "    Xall, Yall = to_torch(Xn, Yn, device=device)\n",
    "    Nall = Xall.shape[0]; Bw = min(batch, Nall)\n",
    "    freezed = list(net.embed.parameters()) + [net.omega] + list(net.trans.parameters())\n",
    "    for pmt in freezed: pmt.requires_grad_(False)\n",
    "    opt_warm = optim.Adam(\n",
    "        list(net.feat.parameters()) + list(net.pi_head.parameters())\n",
    "        + list(net.mu_head.parameters()) + list(net.logsig_head.parameters()),\n",
    "        lr=lr_warm\n",
    "    )\n",
    "    for _ in range(epochs_warm):\n",
    "        idx = torch.randperm(Nall, device=device)\n",
    "        for k0 in range(0, Nall, Bw):\n",
    "            sel = idx[k0:k0+Bw]\n",
    "            pi_logits, mu, sigma = net.gmm_params(Xall[sel])\n",
    "            rho_ones = torch.ones((sel.shape[0],1), device=device)\n",
    "            logp = net.logprob(Yall[sel], pi_logits, mu, sigma, rho_ones)\n",
    "            loss = -logp.mean()\n",
    "            opt_warm.zero_grad(); loss.backward()\n",
    "            if grad_clip: nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n",
    "            opt_warm.step()\n",
    "    # Joint\n",
    "    for pmt in freezed: pmt.requires_grad_(True)\n",
    "    opt_joint = optim.Adam(net.parameters(), lr=lr_joint)\n",
    "    for _ in range(epochs_joint):\n",
    "        idx = torch.randperm(Ntr, device=device)\n",
    "        for k0 in range(0, Ntr, B):\n",
    "            sel = idx[k0:k0+B]\n",
    "            loss, _logs = net.nll_with_aux(\n",
    "                s_policy= Xt[sel],\n",
    "                s_embed = Xt[sel],\n",
    "                a       = At[sel],\n",
    "                demo_ids= Ids_tr[sel],\n",
    "                s_next  = Xnext[sel]\n",
    "            )\n",
    "            opt_joint.zero_grad(); loss.backward()\n",
    "            if grad_clip: nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n",
    "            opt_joint.step()\n",
    "    # Final training loss on transitions (same objective)\n",
    "    with torch.no_grad():\n",
    "        loss, logs = net.nll_with_aux(s_policy=Xt, s_embed=Xt, a=At, demo_ids=Ids_tr, s_next=Xnext)\n",
    "        loss_val = float(loss.item())\n",
    "        components = {k: float(v.item()) for k,v in logs.items()}\n",
    "    return net, loss_val, components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bc_gmm(Xn, Yn, width=128, M=5, lr=1e-3, epochs=120, batch=8192, device=\"cpu\", clip=1.0):\n",
    "    p, d = Xn.shape[1], Yn.shape[1]\n",
    "    net = GMMMLP(p, d, M=M, width=width).to(device)\n",
    "    opt = optim.Adam(net.parameters(), lr=lr)\n",
    "    Xt, Yt = to_torch(Xn, Yn, device=device)\n",
    "    N = Xt.shape[0]; B = min(batch, N)\n",
    "    for _ in range(epochs):\n",
    "        idx = torch.randperm(N, device=device)\n",
    "        for k in range(0, N, B):\n",
    "            sel = idx[k:k+B]; x = Xt[sel]; y = Yt[sel]\n",
    "            dist = net(x)\n",
    "            loss = -dist.log_prob(y).mean()\n",
    "            opt.zero_grad(); loss.backward()\n",
    "            if clip: nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "    with torch.no_grad():\n",
    "        dist = net(Xt)\n",
    "        loss = -dist.log_prob(Yt).mean().item()\n",
    "    return net, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "import torchdiffeq as ode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Train & report ----------------\n",
    "p, d = Xn.shape[1], Yn.shape[1]\n",
    "t0 = time.time()\n",
    "bc_net, bc_loss = train_bc(Xn, Yn, epochs=120, device=device)\n",
    "t1 = time.time()\n",
    "traj_net, traj_loss, W_traj = train_traj_bc(demos, Xn, Yn, epochs=120, device=device)\n",
    "t2 = time.time()\n",
    "ileed_net, ileed_loss, ileed_logs = train_ileed_paperfaithful(\n",
    "    demos, Xn, Yn, demo_ids, epochs_warm=60, epochs_joint=90, device=device\n",
    ")\n",
    "t3 = time.time()\n",
    "bc_gmm_net, bc_gmm_loss = train_bc_gmm(Xn, Yn, epochs=120, device=device)\n",
    "t4 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    {\"model\": \"BC (MLP)\", \"train_objective\": \"Weighted MSE (here: unweighted)\", \"final_loss\": bc_loss, \"time_s\": t1-t0},\n",
    "    {\"model\": \"Traj-BC\", \"train_objective\": \"Weighted MSE (per-trajectory weights)\", \"final_loss\": traj_loss, \"time_s\": t2-t1},\n",
    "    {\"model\": \"ILEED \", \"train_objective\": \"NLL + aux*lambda + reg\", \"final_loss\": ileed_loss, \"time_s\": t3-t2,\n",
    "     **{f\"ILEED_{k}\": v for k,v in ileed_logs.items()}},\n",
    "    {\"model\": \"BC-GMM\", \"train_objective\": \"Negative Log-Likelihood\", \"final_loss\": bc_gmm_loss, \"time_s\": t4-t3},\n",
    "]\n",
    "\n",
    "\n",
    "# Display the updated results table and save to CSV\n",
    "# df = pd.DataFrame(results)\n",
    "# out_csv = \"newlosses_summary_with_node.csv\"\n",
    "# df.to_csv(out_csv, index=False)\n",
    "# print(\"Saved:\", out_csv)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(results)\n",
    "# out_csv = \"newlosses_summary_noisy.csv\"\n",
    "# df.to_csv(out_csv, index=False)\n",
    "# print(\"Saved:\", out_csv)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trajectory generation/Rollout code starts below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_policy(model, stats, init_state, T=50, model_type=\"bc\", device=\"cpu\"):\n",
    "\n",
    "\n",
    "    x_mean, x_std = stats[\"x_mean\"], stats[\"x_std\"]\n",
    "    y_mean, y_std = stats[\"y_mean\"], stats[\"y_std\"]\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "\n",
    "    # normalize initial state\n",
    "    s = (init_state - x_mean) / x_std\n",
    "    s_torch = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # For sequence models (RNN, Transformer)\n",
    "    # if model_type in [\"rnn\", \"bc_transformer\"]:\n",
    "    #     seq_len = 32  # This should match the training seq_len\n",
    "    #     state_sequence = [s_torch]\n",
    "    #     if model_type == \"rnn\":\n",
    "    #         h = None\n",
    "\n",
    "    for t in range(T):\n",
    "        with torch.no_grad():\n",
    "            if model_type in [\"bc\", \"traj_bc\"]:\n",
    "                a_norm = model(s_torch).cpu().numpy()[0]\n",
    "            \n",
    "            elif model_type == \"bc_gmm\":\n",
    "                dist = model(s_torch)\n",
    "                a_norm = dist.mean.cpu().numpy()[0]\n",
    "\n",
    "            elif model_type == \"ileed\":\n",
    "                pi_logits, mu, sigma = model.gmm_params(s_torch)\n",
    "                rho = torch.ones((1,1), device=device)\n",
    "                logpi = torch.log_softmax(pi_logits, dim=-1)\n",
    "               \n",
    "                k = torch.argmax(logpi, dim=-1).item()\n",
    "                a_norm = mu[0, k].cpu().numpy()\n",
    "        \n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model_type {model_type}\")\n",
    "\n",
    "        # unnormalize action\n",
    "        a = a_norm * y_std + y_mean\n",
    "\n",
    "        # store\n",
    "        states.append(s * x_std + x_mean)  # unnormalized\n",
    "        actions.append(a)\n",
    "\n",
    "        # simple dynamics assumption: next state = current state + action * dt\n",
    "        dt = 0.1\n",
    "        s_next = s + (a / x_std) * dt  \n",
    "\n",
    "        # prepare for next step\n",
    "        s = s_next\n",
    "        s_torch = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        # for sequence models, append the new state to the context\n",
    "        # if model_type in [\"rnn\", \"bc_transformer\"]:\n",
    "        #     state_sequence.append(s_torch)\n",
    "\n",
    "    return np.array(states), np.array(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the display names to the exact model_type strings used in rollout_policy\n",
    "MODEL_TYPE_MAP = {\n",
    "    \"BC\": \"bc\",\n",
    "    \"Traj-BC\": \"traj_bc\",\n",
    "    \"ILEED\": \"ileed\",\n",
    "    \"BC-GMM\": \"bc_gmm\",\n",
    "}\n",
    "\n",
    "def perform_multiple_rollouts_and_save(model, stats, T, num_rollouts, display_name, device):\n",
    "    \n",
    "    print(f\"Generating {num_rollouts} rollouts for {display_name}...\")\n",
    "    all_trajectories = []\n",
    "    \n",
    "    base_init_state = X[0]\n",
    "    \n",
    "    delta_max = 0.2\n",
    "    \n",
    "    model_type = MODEL_TYPE_MAP.get(display_name)\n",
    "    if not model_type:\n",
    "        raise ValueError(f\"Display name '{display_name}' not found in model type map.\")\n",
    "\n",
    "    for i in range(num_rollouts):\n",
    "        # Add a random delta to the base initial state\n",
    "        random_delta = np.random.uniform(-delta_max, delta_max, size=base_init_state.shape)\n",
    "        init_state = base_init_state + random_delta\n",
    "\n",
    "        states, actions = rollout_policy(model, stats, init_state, T=T, model_type=model_type, device=device)\n",
    "        all_trajectories.append({\"states\": states, \"actions\": actions})\n",
    "\n",
    "    save_path = f\"{model_type.replace('_', '-')}_rollouts_noisy.npy\"\n",
    "    np.save(save_path, all_trajectories, allow_pickle=True)\n",
    "    print(f\"Saved {num_rollouts} trajectories to {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "num_rollouts_to_generate = 50\n",
    "rollout_horizon = 60\n",
    "\n",
    "saved_files = {}\n",
    "\n",
    "saved_files[\"BC\"] = perform_multiple_rollouts_and_save(bc_net, stats, rollout_horizon, num_rollouts_to_generate, \"BC\", device)\n",
    "saved_files[\"Traj-BC\"] = perform_multiple_rollouts_and_save(traj_net, stats, rollout_horizon, num_rollouts_to_generate, \"Traj-BC\", device)\n",
    "saved_files[\"ILEED\"] = perform_multiple_rollouts_and_save(ileed_net, stats, rollout_horizon, num_rollouts_to_generate, \"ILEED\", device)\n",
    "saved_files[\"BC-GMM\"] = perform_multiple_rollouts_and_save(bc_gmm_net, stats, rollout_horizon, num_rollouts_to_generate, \"BC-GMM\", device)\n",
    "\n",
    "print(\"\\nAll rollouts saved. The files are:\")\n",
    "for name, path in saved_files.items():\n",
    "    print(f\"- {name}: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define the directory where your .npy files are saved.\n",
    "# REPLACE THIS WITH YOUR ACTUAL PATH.\n",
    "file_directory = \"/path/to/your/saved/rollouts\" \n",
    "\n",
    "# Define the models and their corresponding file name bases.\n",
    "# The filenames are derived from the output you provided.\n",
    "models_to_plot_names = [\n",
    "    \"bc_rollouts_noisy\",\n",
    "    \"traj-bc_rollouts_noisy\",\n",
    "    \"ileed_rollouts_noisy\",\n",
    "    \"bc-gmm_rollouts_noisy\",\n",
    "]\n",
    "\n",
    "for model_name_base in models_to_plot_names:\n",
    "\n",
    "    file_path = os.path.join(file_directory, model_name_base + \".npy\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        \n",
    "        \n",
    "        model_name = model_name_base.replace(\"-rollouts_noisy\", \"\").replace(\"_\", \" \").replace(\"-\", \" \").upper()\n",
    "\n",
    "      \n",
    "        plt.title(f\"{model_name} Rolled-out Trajectories\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.xlabel(\"X axis\", fontsize=12, fontweight=\"bold\")\n",
    "        plt.ylabel(\"Y axis\", fontsize=12, fontweight=\"bold\")\n",
    "        plt.xticks(fontsize=10, fontweight=\"bold\")\n",
    "        plt.yticks(fontsize=10, fontweight=\"bold\")\n",
    "        plt.xlim(-2, 2)\n",
    "        plt.ylim(-2, 2)\n",
    "        plt.grid(True, linewidth=1.2)\n",
    "        plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "        # Load the array of dictionaries from the .npy file\n",
    "        all_rollouts = np.load(file_path, allow_pickle=True)\n",
    "        print(f\"Loaded {len(all_rollouts)} rollouts from {file_path}\")\n",
    "\n",
    "        # Generate a colormap with enough colors for all rollouts\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(all_rollouts)))\n",
    "        for i, rollout in enumerate(all_rollouts):\n",
    "            states = rollout[\"states\"]\n",
    "            plt.plot(\n",
    "                states[:, 0],\n",
    "                states[:, 1],\n",
    "                alpha=0.8,\n",
    "                linewidth=1.5,\n",
    "                color=colors[i] \n",
    "            )\n",
    "            \n",
    "        output_file = f\"rollout_plot_{model_name.lower().replace(' ', '-')}.png\"\n",
    "        plt.savefig(output_file, dpi=600, bbox_inches=\"tight\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
